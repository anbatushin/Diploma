{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7d8c3051-a53e-4105-89e7-20a76e5b5e98",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "usage: ipykernel_launcher.py [-h] [--lr LR] [--gamma GAMMA]\n",
      "                             [--gae_lambda GAE_LAMBDA]\n",
      "                             [--eps_clip EPS_CLIP] [--epochs EPOCHS]\n",
      "                             [--minibatch_size MINIBATCH_SIZE]\n",
      "                             [--batch_size BATCH_SIZE]\n",
      "                             [--max_episodes MAX_EPISODES] [--record]\n",
      "ipykernel_launcher.py: error: unrecognized arguments: -f C:\\Users\\anbat\\AppData\\Roaming\\jupyter\\runtime\\kernel-ea1dc42a-050d-486c-88ea-c03257af6a46.json\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[31mSystemExit\u001b[39m\u001b[31m:\u001b[39m 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\anbat\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\IPython\\core\\interactiveshell.py:3557: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import argparse\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.distributions import Categorical\n",
    "import gym\n",
    "from gym.wrappers import RecordVideo\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.animation import FuncAnimation\n",
    "matplotlib.use('TkAgg')  # 使用TkAgg后端确保跨平台兼容性\n",
    "\n",
    "# 设备配置\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# 神经网络模块\n",
    "class ActorCritic(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim):\n",
    "        super().__init__()\n",
    "        self.shared = nn.Sequential(\n",
    "            nn.Linear(state_dim, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 256),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.actor = nn.Linear(256, action_dim)\n",
    "        self.critic = nn.Linear(256, 1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.shared(x)\n",
    "        logits = self.actor(x)\n",
    "        value = self.critic(x)\n",
    "        return logits, value.squeeze(-1)\n",
    "    \n",
    "    def get_action(self, state):\n",
    "        with torch.no_grad():\n",
    "            logits, value = self(state)\n",
    "        dist = Categorical(logits=logits)\n",
    "        action = dist.sample()\n",
    "        return action.item(), dist.log_prob(action)\n",
    "\n",
    "# 经验回放缓冲区\n",
    "class ExperienceBuffer:\n",
    "    def __init__(self, capacity, state_dim):\n",
    "        self.states = np.zeros((capacity, state_dim), dtype=np.float32)\n",
    "        self.actions = np.zeros(capacity, dtype=np.int64)\n",
    "        self.rewards = np.zeros(capacity, dtype=np.float32)\n",
    "        self.values = np.zeros(capacity, dtype=np.float32)\n",
    "        self.log_probs = np.zeros(capacity, dtype=np.float32)\n",
    "        self.terminateds = np.zeros(capacity, dtype=bool)\n",
    "        self.truncateds = np.zeros(capacity, dtype=bool)\n",
    "        self.ptr = 0\n",
    "        self.capacity = capacity\n",
    "        \n",
    "    def store(self, state, action, reward, value, log_prob, terminated, truncated):\n",
    "        idx = self.ptr % self.capacity\n",
    "        self.states[idx] = state\n",
    "        self.actions[idx] = action\n",
    "        self.rewards[idx] = reward\n",
    "        self.values[idx] = value\n",
    "        self.log_probs[idx] = log_prob\n",
    "        self.terminateds[idx] = terminated\n",
    "        self.truncateds[idx] = truncated\n",
    "        self.ptr += 1\n",
    "        \n",
    "    def get(self):\n",
    "        return (\n",
    "            torch.tensor(self.states[:self.ptr], device=device),\n",
    "            torch.tensor(self.actions[:self.ptr], device=device),\n",
    "            torch.tensor(self.rewards[:self.ptr], device=device),\n",
    "            torch.tensor(self.values[:self.ptr], device=device),\n",
    "            torch.tensor(self.log_probs[:self.ptr], device=device),\n",
    "            self.terminateds[:self.ptr],\n",
    "            self.truncateds[:self.ptr]\n",
    "        )\n",
    "    \n",
    "    def clear(self):\n",
    "        self.ptr = 0\n",
    "\n",
    "# PPO训练器\n",
    "class PPOTrainer:\n",
    "    def __init__(self, env, args):\n",
    "        self.env = env\n",
    "        self.args = args\n",
    "        self.model = ActorCritic(env.observation_space.shape[0], \n",
    "                                env.action_space.n).to(device)\n",
    "        self.optimizer = optim.Adam(self.model.parameters(), lr=args.lr)\n",
    "        self.buffer = ExperienceBuffer(args.batch_size, env.observation_space.shape[0])\n",
    "        \n",
    "    def compute_returns_advantages(self, last_value=0):\n",
    "        states, actions, rewards, values, log_probs, terminateds, truncateds = self.buffer.get()\n",
    "        with torch.no_grad():\n",
    "            _, next_value = self.model(states[-1].unsqueeze(0))\n",
    "        \n",
    "        returns = np.zeros(self.buffer.ptr, dtype=np.float32)\n",
    "        advantages = np.zeros(self.buffer.ptr, dtype=np.float32)\n",
    "        last_gae = 0\n",
    "        for t in reversed(range(self.buffer.ptr)):\n",
    "            if t == self.buffer.ptr - 1:\n",
    "                next_non_terminal = 1.0 - (terminateds[t] | truncateds[t])\n",
    "                next_value = last_value\n",
    "            else:\n",
    "                next_non_terminal = 1.0 - (terminateds[t+1] | truncateds[t+1])\n",
    "                next_value = values[t+1]\n",
    "            \n",
    "            delta = rewards[t] + self.args.gamma * next_value * next_non_terminal - values[t]\n",
    "            advantages[t] = last_gae = delta + self.args.gamma * self.args.gae_lambda * next_non_terminal * last_gae\n",
    "            returns[t] = advantages[t] + values[t]\n",
    "        \n",
    "        # 归一化优势\n",
    "        advantages = (advantages - advantages.mean()) / (advantages.std() + 1e-8)\n",
    "        return returns, advantages\n",
    "\n",
    "    def update(self):\n",
    "        states, actions, _, _, old_log_probs, _, _ = self.buffer.get()\n",
    "        returns, advantages = self.compute_returns_advantages()\n",
    "        returns = torch.tensor(returns, device=device)\n",
    "        advantages = torch.tensor(advantages, device=device)\n",
    "        \n",
    "        # 训练多个epoch\n",
    "        for _ in range(self.args.epochs):\n",
    "            indices = torch.randperm(self.buffer.ptr, device=device)\n",
    "            for start in range(0, self.buffer.ptr, self.args.minibatch_size):\n",
    "                end = start + self.args.minibatch_size\n",
    "                idx = indices[start:end]\n",
    "                \n",
    "                # 计算新策略\n",
    "                logits, values = self.model(states[idx])\n",
    "                dist = Categorical(logits=logits)\n",
    "                entropy = dist.entropy().mean()\n",
    "                new_log_probs = dist.log_prob(actions[idx])\n",
    "                \n",
    "                # 重要性采样比率\n",
    "                ratio = (new_log_probs - old_log_probs[idx]).exp()\n",
    "                surr1 = ratio * advantages[idx]\n",
    "                surr2 = torch.clamp(ratio, 1-self.args.eps_clip, 1+self.args.eps_clip) * advantages[idx]\n",
    "                policy_loss = -torch.min(surr1, surr2).mean()\n",
    "                \n",
    "                # 价值损失\n",
    "                value_loss = 0.5 * (values - returns[idx]).pow(2).mean()\n",
    "                \n",
    "                # 总损失\n",
    "                loss = policy_loss + 0.5 * value_loss - 0.01 * entropy\n",
    "                \n",
    "                # 梯度裁剪\n",
    "                self.optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                nn.utils.clip_grad_norm_(self.model.parameters(), 0.5)\n",
    "                self.optimizer.step()\n",
    "                \n",
    "        self.buffer.clear()\n",
    "        return loss.item(), entropy.item()\n",
    "\n",
    "# 实时可视化模块\n",
    "class TrainingVisualizer:\n",
    "    def __init__(self):\n",
    "        self.fig, (self.ax1, self.ax2, self.ax3) = plt.subplots(3, 1, figsize=(8, 9))\n",
    "        self.rewards = []\n",
    "        self.losses = []\n",
    "        self.entropies = []\n",
    "        \n",
    "    def update(self, reward, loss, entropy):\n",
    "        self.rewards.append(reward)\n",
    "        self.losses.append(loss)\n",
    "        self.entropies.append(entropy)\n",
    "        \n",
    "        self.ax1.clear()\n",
    "        self.ax1.plot(self.rewards, color='tab:blue')\n",
    "        self.ax1.set_title('Episode Reward')\n",
    "        \n",
    "        self.ax2.clear()\n",
    "        self.ax2.plot(self.losses, color='tab:orange')\n",
    "        self.ax2.set_title('Training Loss')\n",
    "        \n",
    "        self.ax3.clear()\n",
    "        self.ax3.plot(self.entropies, color='tab:green')\n",
    "        self.ax3.set_title('Policy Entropy')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.pause(0.05)\n",
    "\n",
    "# 训练流程\n",
    "def train(args):\n",
    "    env = gym.make('CartPole-v1')\n",
    "    if args.record:\n",
    "        env = RecordVideo(env, 'video', episode_trigger=lambda x: x % 50 == 0)\n",
    "    \n",
    "    trainer = PPOTrainer(env, args)\n",
    "    visualizer = TrainingVisualizer()\n",
    "    \n",
    "    episode = 0\n",
    "    total_reward = 0\n",
    "    state, _ = env.reset()\n",
    "    state = torch.FloatTensor(state).to(device)\n",
    "    \n",
    "    while episode < args.max_episodes:\n",
    "        # 收集经验\n",
    "        for _ in range(args.batch_size):\n",
    "            action, log_prob = trainer.model.get_action(state)\n",
    "            value = trainer.model(state)[1].item()\n",
    "            \n",
    "            next_state, reward, terminated, truncated, _ = env.step(action)\n",
    "            done = terminated or truncated\n",
    "            \n",
    "            trainer.buffer.store(state.cpu().numpy(), action, reward, \n",
    "                               value, log_prob.item(), terminated, truncated)\n",
    "            \n",
    "            state = torch.FloatTensor(next_state).to(device)\n",
    "            total_reward += reward\n",
    "            \n",
    "            if done:\n",
    "                episode += 1\n",
    "                print(f\"Episode {episode}: Reward {total_reward}\")\n",
    "                \n",
    "                # 更新可视化\n",
    "                loss, entropy = trainer.update()\n",
    "                visualizer.update(total_reward, loss, entropy)\n",
    "                \n",
    "                total_reward = 0\n",
    "                state, _ = env.reset()\n",
    "                state = torch.FloatTensor(state).to(device)\n",
    "                \n",
    "    env.close()\n",
    "    torch.save(trainer.model.state_dict(), 'ppo_cartpole.pth')\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('--lr', type=float, default=3e-4)\n",
    "    parser.add_argument('--gamma', type=float, default=0.99)\n",
    "    parser.add_argument('--gae_lambda', type=float, default=0.95)\n",
    "    parser.add_argument('--eps_clip', type=float, default=0.2)\n",
    "    parser.add_argument('--epochs', type=int, default=10)\n",
    "    parser.add_argument('--minibatch_size', type=int, default=64)\n",
    "    parser.add_argument('--batch_size', type=int, default=2048)\n",
    "    parser.add_argument('--max_episodes', type=int, default=300)\n",
    "    parser.add_argument('--record', action='store_true')\n",
    "    args = parser.parse_args()\n",
    "    \n",
    "    train(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3ce8e7d1-5fd9-4180-98c2-abb322d76653",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'plot_rewards_comparison' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mplot_rewards_comparison\u001b[49m(episode_rewards)\n",
      "\u001b[31mNameError\u001b[39m: name 'plot_rewards_comparison' is not defined"
     ]
    }
   ],
   "source": [
    "plot_rewards_comparison(episode_rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4018de20-33c4-4255-b850-36a076158d1e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
